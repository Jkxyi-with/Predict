{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ad4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c88637",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/root/lanyun-tmp/prediction/UserBehavior.csv\"\n",
    "output_path = \"/root/lanyun-tmp/prediction/train.csv\"\n",
    "last_day = datetime(2017, 12, 1, 23, 59, 59) \n",
    "required_days = 4\n",
    "batch_size = 50000\n",
    "tmp_folder = \"./tmp_batches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1436d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ åŠ è½½æ•°æ®...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ åŠ è½½æ•°æ®...\")\n",
    "df = pd.read_csv(input_path, header=None,\n",
    "                 names=[\"user_id\", \"item_id\", \"category_id\", \"behavior_type\", \"timestamp\"])\n",
    "df[\"time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "df = df[df[\"time\"] <= last_day].copy()\n",
    "df = df.sort_values(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f98687",
   "metadata": {},
   "source": [
    "Train & Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793767aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ç­›é€‰åœ¨ last_day å‰å‘ç”Ÿçš„æ‰€æœ‰è¡Œä¸ºè®°å½• ===\n",
    "used_df = df[df[\"time\"] <= last_day].copy()\n",
    "\n",
    "# === è·å–æ¯ä¸ªç”¨æˆ·æœ€åä¸€æ¬¡æ´»è·ƒæ—¶é—´ ===\n",
    "print(\"ğŸ” åˆ¤æ–­å“ªäº›ç”¨æˆ·åœ¨æœ€åä¸€å¤©å‰å°±å·²è¿ç»­ â‰¥ N å¤©æ— è¡Œä¸º...\")\n",
    "last_active = used_df.groupby(\"user_id\")[\"time\"].max()\n",
    "event_cutoff_time = last_day - timedelta(days=required_days)\n",
    "\n",
    "# å¦‚æœæœ€åæ´»è·ƒæ—¶é—´ â‰¤ æˆªæ­¢æ—¶é—´ â‡’ è¡¨æ˜ä» cutoff åç”¨æˆ·å°±å†ä¹Ÿæ²¡æœ‰è¡Œä¸º â‡’ event=1\n",
    "event_series = (last_active <= event_cutoff_time).astype(int).rename(\"event\")\n",
    "\n",
    "# === ç‰¹å¾ç»Ÿè®¡ï¼šduration å’Œè¡Œä¸ºç±»å‹ ===\n",
    "print(\"ğŸ“Š è®¡ç®—ç”¨æˆ·ç‰¹å¾ï¼ˆduration + è¡Œä¸ºåˆ†å¸ƒï¼‰...\")\n",
    "user_span = used_df.groupby(\"user_id\")[\"time\"].agg([\"min\", \"max\"]).reset_index()\n",
    "user_span[\"duration\"] = (user_span[\"max\"] - user_span[\"min\"]).dt.days\n",
    "user_span = user_span[[\"user_id\", \"duration\"]]\n",
    "\n",
    "features = used_df.groupby(\"user_id\")[\"behavior_type\"].value_counts().unstack().fillna(0).reset_index()\n",
    "\n",
    "# === åˆå¹¶ä¿å­˜ ===\n",
    "print(\"ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆè®­ç»ƒæ•°æ®...\")\n",
    "cox_df = user_span.merge(event_series, on=\"user_id\").merge(features, on=\"user_id\", how=\"left\")\n",
    "cox_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === è¾“å‡ºç»Ÿè®¡ ===\n",
    "print(f\"âœ… å·²ä¿å­˜ï¼š{output_path}\")\n",
    "print(f\"âœ… æµå¤±ç”¨æˆ·æ•° (event=1): {(cox_df['event'] == 1).sum()} / {len(cox_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f48d0",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹å­˜æ‰¹æ¬¡ç»“æœ ===\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "# === è·å–æ‰€æœ‰ç”¨æˆ·å¹¶åˆ†æ‰¹å¤„ç† ===\n",
    "user_ids = df[\"user_id\"].unique()\n",
    "print(f\"ğŸ“¦ å…± {len(user_ids)} ä¸ªç”¨æˆ·ï¼Œå°†åˆ†æ‰¹å¤„ç†æ¯æ‰¹ {batch_size} ä¸ªç”¨æˆ·\")\n",
    "\n",
    "for i in range(0, len(user_ids), batch_size):\n",
    "    batch_user_ids = user_ids[i:i+batch_size]\n",
    "    print(f\"\\nğŸš§ æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}ï¼ˆç”¨æˆ· {i} ~ {i+len(batch_user_ids)-1}ï¼‰...\")\n",
    "\n",
    "    # å½“å‰æ‰¹æ¬¡ç”¨æˆ·çš„æ•°æ®\n",
    "    batch_df = df[df[\"user_id\"].isin(batch_user_ids)].copy()\n",
    "\n",
    "    # è·å–æœ€åæ´»è·ƒæ—¶é—´\n",
    "    last_active = batch_df.groupby(\"user_id\")[\"time\"].max()\n",
    "    event_cutoff_time = last_day - timedelta(days=required_days)\n",
    "    event_series = (last_active <= event_cutoff_time).astype(int).rename(\"event\")\n",
    "\n",
    "    # duration ç»Ÿè®¡\n",
    "    user_span = batch_df.groupby(\"user_id\")[\"time\"].agg([\"min\", \"max\"]).reset_index()\n",
    "    user_span[\"duration\"] = (user_span[\"max\"] - user_span[\"min\"]).dt.days\n",
    "    user_span = user_span[[\"user_id\", \"duration\"]]\n",
    "\n",
    "    # è¡Œä¸ºç±»å‹ç»Ÿè®¡\n",
    "    features = batch_df.groupby(\"user_id\")[\"behavior_type\"].value_counts().unstack().fillna(0).reset_index()\n",
    "\n",
    "    # åˆå¹¶\n",
    "    cox_df = user_span.merge(event_series, on=\"user_id\").merge(features, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    # ä¿å­˜ä¸´æ—¶ç»“æœ\n",
    "    tmp_file = os.path.join(tmp_folder, f\"tmp_batch_{i//batch_size + 1}.csv\")\n",
    "    cox_df.to_csv(tmp_file, index=False)\n",
    "    print(f\"âœ… æ‰¹æ¬¡ {i//batch_size + 1} å·²ä¿å­˜ï¼š{tmp_file}\")\n",
    "\n",
    "# === åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ ===\n",
    "print(\"\\nğŸ”— åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶...\")\n",
    "all_parts = [pd.read_csv(os.path.join(tmp_folder, f)) for f in sorted(os.listdir(tmp_folder)) if f.endswith(\".csv\")]\n",
    "final_df = pd.concat(all_parts, ignore_index=True)\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… æœ€ç»ˆè®­ç»ƒé›†å·²ä¿å­˜ï¼š{output_path}\")\n",
    "print(f\"âœ… æµå¤±ç”¨æˆ·æ•° (event=1): {(final_df['event'] == 1).sum()} / {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc21cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "125eb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ç‰¹å¾æ„å»ºå‡½æ•° ===\n",
    "def add_user_features(df: pd.DataFrame, window_end_date: str, output_path: str, df_all_behavior: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df[\"total_actions\"] = df[[\"pv\", \"cart\", \"fav\", \"buy\"]].sum(axis=1)\n",
    "    df[\"action_days\"] = df[\"duration\"] + 1\n",
    "    df[\"avg_actions_per_day\"] = df[\"total_actions\"] / (df[\"action_days\"] + 1e-5)\n",
    "\n",
    "    def compute_entropy(row):\n",
    "        actions = row[[\"pv\", \"cart\", \"fav\", \"buy\"]].values\n",
    "        probs = actions / (actions.sum() + 1e-5)\n",
    "        return entropy(probs)\n",
    "\n",
    "    df[\"behavior_entropy\"] = df.apply(compute_entropy, axis=1)\n",
    "\n",
    "    # è®¡ç®— recencyï¼ˆä»…é™æˆªæ­¢æ—¥æœŸå‰çš„è¡Œä¸ºï¼‰\n",
    "    cutoff = pd.to_datetime(window_end_date)\n",
    "    last_time = df_all_behavior[df_all_behavior[\"time\"] <= cutoff].groupby(\"user_id\")[\"time\"].max().reset_index()\n",
    "    last_time[\"recency\"] = (cutoff - last_time[\"time\"]).dt.days\n",
    "    df = df.merge(last_time[[\"user_id\", \"recency\"]], on=\"user_id\", how=\"left\")\n",
    "    df[\"recency\"] = df[\"recency\"].fillna(df[\"duration\"])\n",
    "\n",
    "    # ç‰¹å¾å½’ä¸€åŒ–\n",
    "    cols_to_scale = [\"duration\", \"total_actions\", \"action_days\", \"avg_actions_per_day\", \"behavior_entropy\", \"recency\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    df[[col + \"_scaled\" for col in cols_to_scale]] = scaler.fit_transform(df[cols_to_scale])\n",
    "\n",
    "    # ä¿å­˜æ–‡ä»¶\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… ç‰¹å¾å·²ä¿å­˜è‡³ï¼š{output_path}ï¼ŒåŒ…å«ç”¨æˆ·æ•°ï¼š{len(df)}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c093be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç‰¹å¾å·²ä¿å­˜è‡³ï¼šcox_train_with_features.csvï¼ŒåŒ…å«ç”¨æˆ·æ•°ï¼š987984\n",
      "âœ… ç‰¹å¾å·²ä¿å­˜è‡³ï¼šcox_val_with_features.csvï¼ŒåŒ…å«ç”¨æˆ·æ•°ï¼š987992\n",
      "âœ… ç‰¹å¾å·²ä¿å­˜è‡³ï¼šcox_test_with_features.csvï¼ŒåŒ…å«ç”¨æˆ·æ•°ï¼š987994\n",
      "   user_id  duration  event  buy  cart  fav     pv  total_actions  \\\n",
      "0        1         7      0  0.0   0.0  0.0   44.0           44.0   \n",
      "1        2         5      0  3.0   0.0  7.0   27.0           37.0   \n",
      "2        3         6      0  0.0  14.0  5.0   22.0           41.0   \n",
      "3        4         6      0  4.0  13.0  0.0  248.0          265.0   \n",
      "4        5         4      0  0.0   0.0  0.0   58.0           58.0   \n",
      "\n",
      "   action_days  avg_actions_per_day  behavior_entropy  recency  \\\n",
      "0            8             5.499993          0.000000      0.0   \n",
      "1            6             6.166656          0.748626      1.0   \n",
      "2            7             5.857134          0.957549      0.0   \n",
      "3            7            37.857089          0.273240      0.0   \n",
      "4            5            11.599977          0.000000      0.0   \n",
      "\n",
      "   duration_scaled  total_actions_scaled  action_days_scaled  \\\n",
      "0         0.000166              0.054777            0.000166   \n",
      "1         0.000118              0.045860            0.000118   \n",
      "2         0.000142              0.050955            0.000142   \n",
      "3         0.000142              0.336306            0.000142   \n",
      "4         0.000095              0.072611            0.000095   \n",
      "\n",
      "   avg_actions_per_day_scaled  behavior_entropy_scaled  recency_scaled  \n",
      "0                    0.017405                 0.000000        0.000000  \n",
      "1                    0.019514                 0.540019        0.010638  \n",
      "2                    0.018535                 0.690726        0.000000  \n",
      "3                    0.119802                 0.197101        0.000000  \n",
      "4                    0.036709                 0.000000        0.000000  \n"
     ]
    }
   ],
   "source": [
    "# === æ‰§è¡Œæ„å»ºï¼šTrain / Val / Test ===\n",
    "train_df = pd.read_csv(\"data/train_u.csv\")\n",
    "val_df = pd.read_csv(\"data/valid_u.csv\")\n",
    "test_df = pd.read_csv(\"data/test_u.csv\")\n",
    "\n",
    "train_out = add_user_features(train_df, \"2017-12-01\", \"cox_train_with_features.csv\", df)\n",
    "val_out = add_user_features(val_df, \"2017-12-02\", \"cox_val_with_features.csv\", df)\n",
    "test_out = add_user_features(test_df, \"2017-12-03\", \"cox_test_with_features.csv\", df)\n",
    "\n",
    "# ç¤ºä¾‹è¾“å‡º\n",
    "print(train_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a869ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train é›†åˆ: event=1 æ•°é‡ = 20716 / 987984 (2.10%)\n",
      "Val é›†åˆ: event=1 æ•°é‡ = 4495 / 987992 (0.45%)\n",
      "Test é›†åˆ: event=1 æ•°é‡ = 2169 / 987994 (0.22%)\n"
     ]
    }
   ],
   "source": [
    "# === è¾“å‡ºæ¯ä¸ªæ•°æ®é›†ä¸­ event=1 çš„ç”¨æˆ·æ•°é‡å’Œæ¯”ä¾‹ ===\n",
    "for name, df in zip([\"Train\", \"Val\", \"Test\"], [train_out, val_out, test_out]):\n",
    "    event1 = (df[\"event\"] == 1).sum()\n",
    "    total = len(df)\n",
    "    print(f\"{name} é›†åˆ: event=1 æ•°é‡ = {event1} / {total} ({event1/total:.2%})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
